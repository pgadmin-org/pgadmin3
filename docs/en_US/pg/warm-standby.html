<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title></title>
<link rel="stylesheet" href="stylesheet.css" type="text/css">
<link rev="made" href="pgsql-docs@postgresql.org">
<meta name="generator" content="DocBook XSL Stylesheets V1.71.1">
<link rel="start" href="index.html" title="">
<link rel="up" href="backup.html" title="">
<link rel="prev" href="continuous-archiving.html" title="">
<link rel="next" href="migration.html" title="">
<link rel="copyright" href="ln-legalnotice.html" title="">
</head>
<body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="sect1" lang="en">
<div class="titlepage"><div><div><h2 class="title" style="clear: both">
<a name="warm-standby"></a></h2></div></div></div>
<a name="id2649119"></a><a name="id2649127"></a><a name="id2649136"></a><a name="id2649146"></a><a name="id2649155"></a><a name="id2649164"></a><a name="id2649172"></a><p>   Continuous Archiving can be used to create a High Availability (HA)
   cluster configuration with one or more Standby Servers ready to take
   over operations in the case that the Primary Server fails. This
   capability is more widely known as Warm Standby Log Shipping.
  </p>
<p>   The Primary and Standby Server work together to provide this capability,
   though the servers are only loosely coupled. The Primary Server operates
   in Continuous Archiving mode, while the Standby Server operates in a
   continuous Recovery mode, reading the WAL files from the Primary. No
   changes to the database tables are required to enable this capability,
   so it offers a low administration overhead in comparison with other
   replication approaches. This configuration also has a very low
   performance impact on the Primary server.
  </p>
<p>   Directly moving WAL or "log" records from one database server to another
   is typically described as Log Shipping. <span class="productname">PostgreSQL</span>
   implements file-based log shipping, which means that WAL records are batched one file at a time. WAL
   files can be shipped easily and cheaply over any distance, whether it be
   to an adjacent system, another system on the same site or another system
   on the far side of the globe. The bandwidth required for this technique
   varies according to the transaction rate of the Primary Server.
   Record-based Log Shipping is also possible with custom-developed
   procedures, discussed in a later section. Future developments are likely
   to include options for synchronous and/or integrated record-based log
   shipping.
  </p>
<p>   It should be noted that the log shipping is asynchronous, i.e. the
   WAL records are shipped after transaction commit. As a result there
   can be a small window of data loss, should the Primary Server
   suffer a catastrophic failure. The window of data loss is minimised
   by the use of the <code class="varname">archive_timeout</code> parameter,
   which can be set as low as a few seconds if required. A very low
   setting can increase the bandwidth requirements for file shipping.
  </p>
<p>   The Standby server is not available for access, since it is continually
   performing recovery processing. Recovery performance is sufficiently
   good that the Standby will typically be only minutes away from full
   availability once it has been activated. As a result, we refer to this
   capability as a Warm Standby configuration that offers High
   Availability. Restoring a server from an archived base backup and
   rollforward can take considerably longer and so that technique only
   really offers a solution for Disaster Recovery, not HA.
  </p>
<p>    When running a Standby Server, backups can be performed on the Standby
    rather than the Primary, thereby offloading the expense of
    taking periodic base backups. (See 
    <a href="continuous-archiving.html#backup-incremental-updated" title=""></a>)
   </p>
<p>   Other mechanisms for High Availability replication are available, both
   commercially and as open-source software.  
  </p>
<p>   In general, log shipping between servers running different release
   levels will not be possible. It is the policy of the PostgreSQL Global
   Development Group not to make changes to disk formats during minor release
   upgrades, so it is likely that running different minor release levels 
   on Primary and Standby servers will work successfully. However, no
   formal support for that is offered and you are advised not to allow this
   to occur over long periods.
  </p>
<div class="sect2" lang="en">
<div class="titlepage"><div><div><h3 class="title">
<a name="warm-standby-planning"></a></h3></div></div></div>
<p>    On the Standby server all tablespaces and paths will refer to similarly
    named mount points, so it is important to create the Primary and Standby
    servers so that they are as similar as possible, at least from the
    perspective of the database server. Furthermore, any <a href="sql-createtablespace.html">CREATE TABLESPACE</a>
    commands will be passed across as-is, so any new mount points must be
    created on both servers before they are used on the Primary. Hardware
    need not be the same, but experience shows that maintaining two
    identical systems is easier than maintaining two dissimilar ones over
    the whole lifetime of the application and system.
   </p>
<p>    There is no special mode required to enable a Standby server. The
    operations that occur on both Primary and Standby servers are entirely
    normal continuous archiving and recovery tasks. The primary point of
    contact between the two database servers is the archive of WAL files
    that both share: Primary writing to the archive, Standby reading from
    the archive. Care must be taken to ensure that WAL archives for separate
    servers do not become mixed together or confused.
   </p>
<p>    The magic that makes the two loosely coupled servers work together
    is simply a <code class="varname">restore_command</code> that waits for the next
    WAL file to be archived from the Primary. The <code class="varname">restore_command</code>
    is specified in the <code class="filename">recovery.conf</code> file on the Standby
    Server. Normal recovery processing would request a file from the
    WAL archive, causing an error if the file was unavailable.  For
    Standby processing it is normal for the next file to be
    unavailable, so we must be patient and wait for it to appear. A
    waiting <code class="varname">restore_command</code> can be written as a custom
    script that loops after polling for the existence of the next WAL
    file. There must also be some way to trigger failover, which
    should interrupt the <code class="varname">restore_command</code>, break the loop
    and return a file not found error to the Standby Server. This then
    ends recovery and the Standby will then come up as a normal
    server.
   </p>
<p>    Sample code for the C version of the <code class="varname">restore_command</code>
    would be be:
</p>
<pre class="programlisting">triggered = false;
while (!NextWALFileReady() &amp;&amp; !triggered)
{
    sleep(100000L);         /* wait for ~0.1 sec */
    if (CheckForExternalTrigger())
        triggered = true;
}
if (!triggered)
        CopyWALFileForRecovery();</pre>
<p>
   </p>
<p>    <span class="productname">PostgreSQL</span> does not provide the system
    software required to identify a failure on the Primary and notify
    the Standby system and then the Standby database server. Many such
    tools exist and are well integrated with other aspects of a system
    failover, such as IP address migration.
   </p>
<p>    Triggering failover is an important part of planning and
    design. The <code class="varname">restore_command</code> is executed in full once
    for each WAL file. The process running the <code class="varname">restore_command</code>
    is therefore created and dies for each file, so there is no daemon
    or server process and so we cannot use signals and a signal
    handler. A more permanent notification is required to trigger the
    failover. It is possible to use a simple timeout facility,
    especially if used in conjunction with a known
    <code class="varname">archive_timeout</code> setting on the Primary. This is
    somewhat error prone since a network or busy Primary server might
    be sufficient to initiate failover. A notification mechanism such
    as the explicit creation of a trigger file is less error prone, if
    this can be arranged.
   </p>
</div>
<div class="sect2" lang="en">
<div class="titlepage"><div><div><h3 class="title">
<a name="warm-standby-config"></a></h3></div></div></div>
<p>    The short procedure for configuring a Standby Server is as follows. For
    full details of each step, refer to previous sections as noted.
    </p>
<div class="orderedlist"><ol type="1">
<li><p>       Setup Primary and Standby systems as near identically as
       possible, including two identical copies of
       <span class="productname">PostgreSQL</span> at the same release level.
      </p></li>
<li><p>       Setup Continuous Archiving from the Primary to a WAL archive located
       in a directory on the Standby Server. Ensure that both <a href="runtime-config-wal.html#guc-archive-command">archive_command</a> and <a href="runtime-config-wal.html#guc-archive-timeout">archive_timeout</a>
       are set. (See <a href="continuous-archiving.html#backup-archiving-wal" title=""></a>)
      </p></li>
<li><p>       Make a Base Backup of the Primary Server. (See <a href="continuous-archiving.html#backup-base-backup" title=""></a>)
      </p></li>
<li><p>       Begin recovery on the Standby Server from the local WAL
       archive, using a <code class="filename">recovery.conf</code> that specifies a
       <code class="varname">restore_command</code> that waits as described
       previously. (See <a href="continuous-archiving.html#backup-pitr-recovery" title=""></a>)
      </p></li>
</ol></div>
<p>
   </p>
<p>    Recovery treats the WAL Archive as read-only, so once a WAL file has
    been copied to the Standby system it can be copied to tape at the same
    time as it is being used by the Standby database server to recover.
    Thus, running a Standby Server for High Availability can be performed at
    the same time as files are stored for longer term Disaster Recovery
    purposes. 
   </p>
<p>    For testing purposes, it is possible to run both Primary and Standby
    servers on the same system. This does not provide any worthwhile
    improvement on server robustness, nor would it be described as HA.
   </p>
</div>
<div class="sect2" lang="en">
<div class="titlepage"><div><div><h3 class="title">
<a name="warm-standby-failover"></a></h3></div></div></div>
<p>    If the Primary Server fails then the Standby Server should begin
    failover procedures.
   </p>
<p>    If the Standby Server fails then no failover need take place. If the
    Standby Server can be restarted, even some time later, then the recovery
    process can also be immediately restarted, taking advantage of 
    Restartable Recovery. If the Standby Server cannot be restarted, then a
    full new Standby Server should be created.
   </p>
<p>    If the Primary Server fails and then immediately restarts, you must have
    a mechanism for informing it that it is no longer the Primary. This is
    sometimes known as STONITH (Shoot the Other Node In The Head), which is
    necessary to avoid situations where both systems think they are the
    Primary, which can lead to confusion and ultimately data loss.
   </p>
<p>    Many failover systems use just two systems, the Primary and the Standby,
    connected by some kind of heartbeat mechanism to continually verify the
    connectivity between the two and the viability of the Primary. It is
    also possible to use a third system, known as a Witness Server to avoid
    some problems of inappropriate failover, but the additional complexity
    may not be worthwhile unless it is set-up with sufficient care and
    rigorous testing.
   </p>
<p>    At the instant that failover takes place to the Standby, we have only a
    single server in operation. This is known as a degenerate state.
    The former Standby is now the Primary, but the former Primary is down 
    and may stay down. We must now fully recreate a Standby server, 
    either on the former Primary system when it comes up, or on a third, 
    possibly new, system. Once complete the Primary and Standby can be 
    considered to have switched roles. Some people choose to use a third 
    server to provide additional protection across the failover interval, 
    though clearly this complicates the system configuration and 
    operational processes (and this can also act as a Witness Server).
   </p>
<p>    So, switching from Primary to Standby Server can be fast but requires
    some time to re-prepare the failover cluster. Regular switching from
    Primary to Standby is encouraged, since it allows the regular downtime
    that each system requires to maintain HA. This also acts as a test of the
    failover mechanism so that it definitely works when you really need it. 
    Written administration procedures are advised.
   </p>
</div>
<div class="sect2" lang="en">
<div class="titlepage"><div><div><h3 class="title">
<a name="warm-standby-record"></a></h3></div></div></div>
<p>    The main features for Log Shipping in this release are based
    around the file-based Log Shipping described above. It is also
    possible to implement record-based Log Shipping using the
    <code class="function">pg_xlogfile_name_offset()</code> function (see <a href="functions-admin.html" title=""></a>), though this requires custom
    development.
   </p>
<p>    An external program can call <code class="function">pg_xlogfile_name_offset()</code>
    to find out the file name and the exact byte offset within it of
    the latest WAL pointer.  If the external program regularly polls
    the server it can find out how far forward the pointer has
    moved. It can then access the WAL file directly and copy those
    bytes across to a less up-to-date copy on a Standby Server.
   </p>
</div>
</div></body>
</html>
